{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from model import SimCSEModel\n",
    "from dataset import SimCSEDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 32\n",
    "CHECKPOINT_PATH = './checkpoint/model_epoch_3.pth'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "dataset = SimCSEDataset('data/simple_corpus.txt', tokenizer, max_len=MAX_LEN)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = SimCSEModel(MODEL_NAME).cuda()\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(dataloader):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            emb = model(input_ids, attention_mask=attention_mask)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all embeddings\n",
    "embeddings = get_embeddings(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss index creation\n",
    "dim = embeddings.shape[1]  # dimension of the embeddings\n",
    "index = faiss.IndexFlatL2(dim)  # Use L2 distance for similarity\n",
    "index.add(embeddings)  # Add embeddings to the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, k=5):\n",
    "    query_tokens = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    query_input_ids = query_tokens['input_ids'].cuda()\n",
    "    query_attention_mask = query_tokens['attention_mask'].cuda()\n",
    "\n",
    "    query_embedding = model(query_input_ids, attention_mask=query_attention_mask).cpu().numpy()\n",
    "\n",
    "    # Search in the Faiss index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I love you\"\n",
    "distances, indices = search(query)\n",
    "\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Rank {i + 1}: {dataset[idx]} | Distance: {distances[0][i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
